{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consideramos el problema de transporte:\n",
    "\n",
    "$$\\phi_t+c\\cdot \\phi_x=0 \\hspace{0.8em} (t,x) \\in (0,1)^2$$\n",
    "$$\\phi(t,0)=0; \\phi(t,1)=0 \\hspace{0.8em} t \\in (0,1)$$\n",
    "$$\\phi(0,x) = \\begin{cases} \n",
    "    0 & \\text{if } x \\leq 1/3 \\\\\n",
    "    1 & \\text{if } 1/3 < x \\leq 2/3 \\\\\n",
    "    0 & \\text{if } x > 2/3 \n",
    "\\end{cases}$$\n",
    "\n",
    "Consideramos $c=1$ en este caso."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Conclusiones\n",
    "\n",
    "Necesitamos definir una función de activación especial, el problema es el salto para abajo de despues con una sigmoide el primer salto se emula perfectamente el segundo, necesitamos encontrar una función de activación o algo similar que permita emular esa forma.\n",
    "\n",
    "https://stats.stackexchange.com/questions/364917/what-is-required-for-neural-network-to-approximate-discontinuous-function\n",
    "\n",
    "-----------\n",
    "\n",
    "¿Queremos intentar ir por?\n",
    "\n",
    "I think you can try \"Mixture of Experts\" (MoE) which will help solving a problem by dividing it into smaller, bite-sized pieces, like the brain does\n",
    "\n",
    "The two components of MoE:\n",
    "\n",
    "* The experts: Instead of just one network, we have several mini-networks called \"experts,\" each good at specific parts of the problem.\n",
    "\n",
    "* The gate: Another network, the \"gate,\" decides how much each expert contributes to the final prediction.\n",
    "\n",
    "-> The final prediction is then the weighted summation of expert predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-13 19:48:54.216827: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-04-13 19:48:54.216900: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-04-13 19:48:54.219062: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-04-13 19:48:54.233402: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-13 19:48:55.951715: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "DTYPE='float32'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aux Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_model(num_hidden_layers=8, num_neurons_per_layer=20, add_last_layer=1):\n",
    "    # Initialize a feedforward neural network\n",
    "    model = tf.keras.Sequential()\n",
    "\n",
    "    # Input is two-dimensional (time + one spatial dimension)\n",
    "    model.add(tf.keras.Input([2]))\n",
    "\n",
    "    # Introduce a scaling layer to map input to [lb, ub]\n",
    "    #scaling_layer = tf.keras.layers.Lambda(\n",
    "    #            lambda x: 2.0*(x - lb)/(ub - lb) - 1.0)\n",
    "    #model.add(scaling_layer)\n",
    "\n",
    "    # Append hidden layers\n",
    "    for _ in range(num_hidden_layers):\n",
    "        model.add(tf.keras.layers.Dense(num_neurons_per_layer,\n",
    "            activation=\"sigmoid\", use_bias=True))\n",
    "\n",
    "    # Output is one-dimensional\n",
    "    if(add_last_layer):\n",
    "        model.add(tf.keras.layers.Dense(1, activation=None))\n",
    "        \n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gradient(model, X_r):\n",
    "\n",
    "    # A tf.GradientTape is used to compute derivatives in TensorFlow\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        # Split t and x to compute partial derivatives\n",
    "        x, y = X_r[:, 0:1], X_r[:,1:2]\n",
    "        # Variables t and x are watched during tape\n",
    "        # to compute derivatives u_t and u_x\n",
    "        tape.watch(x)\n",
    "        tape.watch(y)\n",
    "\n",
    "        # Determine residual\n",
    "        u = model(tf.stack([x[:,0], y[:,0]], axis=1))\n",
    "\n",
    "        # Compute gradient u_x within the GradientTape\n",
    "        # since we need second derivatives\n",
    "        u_x = tape.gradient(u, x)\n",
    "        u_y = tape.gradient(u, y)\n",
    "\n",
    "    u_xx = tape.gradient(u_x, x)\n",
    "    u_yy = tape.gradient(u_y, y)\n",
    "\n",
    "    del tape\n",
    "\n",
    "    return fun_r(x,u_x,u_y,u_xx,u_yy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def symmetry(x):\n",
    "    aux = x.numpy()\n",
    "\n",
    "    aux[:, 1] = 2\n",
    "    return tf.convert_to_tensor(aux)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(model, X_r, periodic_points, t0_points):\n",
    "\n",
    "    # Compute phi^r\n",
    "    r  = get_gradient(model, X_r)\n",
    "    phi_r = tf.reduce_mean(tf.square(r))\n",
    "\n",
    "    # Initialize loss\n",
    "    loss_XR = phi_r\n",
    "    \n",
    "    # Periodic \n",
    "    \n",
    "    loss_boundary = tf.reduce_mean(tf.square(model(periodic_points) - model(symmetry(periodic_points))))*0\n",
    "    # TN Point DUDAS\n",
    "    \n",
    "    loss_T0 = tf.reduce_mean(tf.square(model(t0_points)-eval))\n",
    "    \n",
    "    #x, y, u_x, u_y, u_xx, u_yy = get_gradient(model, tn_points)\n",
    "    #tn_error=tf.pow(tf.square(u_x)+ tf.square(u_y), 1/2)-g(x,y)\n",
    "    #tn_error=tf.reduce_sum(tf.square(tn_error), axis=0)\n",
    "    #loss += tn_error\n",
    "    loss = loss_XR + loss_boundary+loss_T0\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_grad(model, X_r, td_points, tn_points):\n",
    "\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        # This tape is for derivatives with\n",
    "        # respect to trainable variables\n",
    "        tape.watch(model.trainable_variables)\n",
    "        loss = compute_loss(model, X_r, td_points, tn_points)\n",
    "\n",
    "    g = tape.gradient(loss, model.trainable_variables)\n",
    "    del tape\n",
    "    \n",
    "    return loss, g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "def train(mlp):\n",
    "    # Define one training step as a TensorFlow function to increase speed of training\n",
    "    @tf.function\n",
    "    def train_step():\n",
    "        # Compute current loss and gradient w.r.t. parameters\n",
    "        loss, grad_theta = get_grad(mlp, X_r, bound_points, t0_points)\n",
    "\n",
    "        # Perform gradient descent step\n",
    "        optim.apply_gradients(zip(grad_theta, mlp.trainable_variables))\n",
    "\n",
    "        return loss\n",
    "\n",
    "    # Number of training epochs\n",
    "    N = 20000\n",
    "    hist = []\n",
    "\n",
    "    # Start timer\n",
    "    t0 = time()\n",
    "\n",
    "    for i in range(N+1):\n",
    "\n",
    "        loss = train_step()\n",
    "\n",
    "        # Append current loss to hist\n",
    "        hist.append(loss.numpy())\n",
    "\n",
    "        # Output current loss after 50 iterates\n",
    "        if i%50 == 0:\n",
    "            print('It {:05d}'.format(i))\n",
    "            print('loss: ', loss.numpy())\n",
    "\n",
    "    # Print computation time\n",
    "    print('\\nComputation time: {} seconds'.format(time()-t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial Condition\n",
    "\n",
    "\n",
    "C=1000\n",
    "def f(x):\n",
    "    return 1/(1+np.exp(-C*(x-1/2)))\n",
    "\n",
    "\n",
    "C=700\n",
    "def exact_sol(m):\n",
    "    t=m[:,0]\n",
    "    x=m[:,1]\n",
    "    C=1500\n",
    "    return 1/(1+np.exp(-C*x+C*t+C/2))\n",
    "\n",
    "\n",
    "'''\n",
    "def f(x):\n",
    "    # Perform the comparison\n",
    "    comparison = tf.logical_and(tf.greater(x, 1/3), tf.less(x, 2/3))\n",
    "\n",
    "    # Convert boolean values to integers (1 for True, 0 for False)\n",
    "    result = tf.cast(comparison, tf.float32)\n",
    "    return result\n",
    "\n",
    "def exact_sol(m):\n",
    "    result = []\n",
    "    for value in m:\n",
    "        if 1/3 < value < 2/3:\n",
    "            result.append(1)\n",
    "        else:\n",
    "            result.append(0)\n",
    "    return result\n",
    "'''\n",
    "\n",
    "# Condition for the equation\n",
    "c=1\n",
    "def fun_r(x, u_x, u_t, u_xx, u_tt):\n",
    "    return u_t+c*u_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "\n",
    "# Number of Data points\n",
    "N_t0=1000 # For initial condition\n",
    "N_periodic=1000 # For periodic Condition\n",
    "N_r=3000 # For Random points inside the domain\n",
    "\n",
    "# Rectangular domain for which we are solving\n",
    "xmin=0\n",
    "xmax=2\n",
    "tmin=0\n",
    "tmax=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-13 19:49:26.146964: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:274] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "/home/alexpf/Projects/ED-Solve/.venv/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py:460: RuntimeWarning: overflow encountered in exp\n",
      "  return f(*args)\n"
     ]
    }
   ],
   "source": [
    "# Creation of points\n",
    "\n",
    "# Lower bounds\n",
    "lb = tf.constant([xmin, tmin], dtype=DTYPE)\n",
    "# Upper bounds\n",
    "ub = tf.constant([xmax, tmax], dtype=DTYPE)\n",
    "\n",
    "# Draw uniformly sampled collocation points\n",
    "x_r = tf.random.uniform((N_r,1), lb[0], ub[0], dtype=DTYPE)\n",
    "y_r = tf.random.uniform((N_r,1), lb[1], ub[1], dtype=DTYPE)\n",
    "\n",
    "X_r = tf.concat([x_r, y_r], axis=1)\n",
    "\n",
    "# t0\n",
    "aux = tf.random.uniform((N_t0,1), lb[0], ub[0], dtype=DTYPE)\n",
    "aux1 = tf.ones((N_t0, 1), dtype=DTYPE)*lb[0]\n",
    "\n",
    "eval = tf.map_fn(f, aux)\n",
    "t0_points=tf.concat([aux1, aux], axis=1)\n",
    "\n",
    "\n",
    "# boundary\n",
    "aux = tf.random.uniform((N_periodic,1), lb[1], ub[1], dtype=DTYPE)\n",
    "aux1 = tf.ones((N_periodic, 1), dtype=DTYPE)*lb[0]\n",
    "bound_points = tf.concat([aux, aux1], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_21 (Dense)            (None, 2)                 6         \n",
      "                                                                 \n",
      " dense_22 (Dense)            (None, 1)                 3         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 9 (36.00 Byte)\n",
      "Trainable params: 9 (36.00 Byte)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It 00000\n",
      "loss:  0.6447771\n",
      "It 00050\n",
      "loss:  0.05434917\n",
      "It 00100\n",
      "loss:  0.020755757\n",
      "It 00150\n",
      "loss:  0.016783234\n",
      "It 00200\n",
      "loss:  0.014363428\n",
      "It 00250\n",
      "loss:  0.012712341\n",
      "It 00300\n",
      "loss:  0.011497286\n",
      "It 00350\n",
      "loss:  0.010557203\n",
      "It 00400\n",
      "loss:  0.009803164\n",
      "It 00450\n",
      "loss:  0.009181298\n",
      "It 00500\n",
      "loss:  0.008657004\n",
      "It 00550\n",
      "loss:  0.008206999\n",
      "It 00600\n",
      "loss:  0.00781503\n",
      "It 00650\n",
      "loss:  0.007469373\n",
      "It 00700\n",
      "loss:  0.0071613495\n",
      "It 00750\n",
      "loss:  0.0068843896\n",
      "It 00800\n",
      "loss:  0.0066334233\n",
      "It 00850\n",
      "loss:  0.0064044623\n",
      "It 00900\n",
      "loss:  0.006194329\n",
      "It 00950\n",
      "loss:  0.0060004573\n",
      "It 01000\n",
      "loss:  0.0076067382\n",
      "It 01050\n",
      "loss:  0.0056646145\n",
      "It 01100\n",
      "loss:  0.005498215\n",
      "It 01150\n",
      "loss:  0.005351879\n",
      "It 01200\n",
      "loss:  0.0052142516\n",
      "It 01250\n",
      "loss:  0.0050845044\n",
      "It 01300\n",
      "loss:  0.0053971075\n",
      "It 01350\n",
      "loss:  0.0048468173\n",
      "It 01400\n",
      "loss:  0.004735236\n",
      "It 01450\n",
      "loss:  0.0046300855\n",
      "It 01500\n",
      "loss:  0.0045297807\n",
      "It 01550\n",
      "loss:  0.010403268\n",
      "It 01600\n",
      "loss:  0.0043965518\n",
      "It 01650\n",
      "loss:  0.0042547383\n",
      "It 01700\n",
      "loss:  0.0041704015\n",
      "It 01750\n",
      "loss:  0.0040893783\n",
      "It 01800\n",
      "loss:  0.0040114005\n",
      "It 01850\n",
      "loss:  0.0043605575\n",
      "It 01900\n",
      "loss:  0.0038672409\n",
      "It 01950\n",
      "loss:  0.003794169\n",
      "It 02000\n",
      "loss:  0.0037266328\n",
      "It 02050\n",
      "loss:  0.0036612696\n",
      "It 02100\n",
      "loss:  0.0035979524\n",
      "It 02150\n",
      "loss:  0.014409618\n",
      "It 02200\n",
      "loss:  0.003491472\n",
      "It 02250\n",
      "loss:  0.0034200153\n",
      "It 02300\n",
      "loss:  0.0033637413\n",
      "It 02350\n",
      "loss:  0.0033093288\n",
      "It 02400\n",
      "loss:  0.003256376\n",
      "It 02450\n",
      "loss:  0.0032048137\n",
      "It 02500\n",
      "loss:  0.019363565\n",
      "It 02550\n",
      "loss:  0.0031433399\n",
      "It 02600\n",
      "loss:  0.0030586002\n",
      "It 02650\n",
      "loss:  0.003011842\n",
      "It 02700\n",
      "loss:  0.0029665348\n",
      "It 02750\n",
      "loss:  0.0029222737\n",
      "It 02800\n",
      "loss:  0.002890081\n",
      "It 02850\n",
      "loss:  0.0031756712\n",
      "It 02900\n",
      "loss:  0.0027961875\n",
      "It 02950\n",
      "loss:  0.002755508\n",
      "It 03000\n",
      "loss:  0.0027161085\n",
      "It 03050\n",
      "loss:  0.0026775291\n",
      "It 03100\n",
      "loss:  0.0026397356\n",
      "It 03150\n",
      "loss:  0.011423751\n",
      "It 03200\n",
      "loss:  0.0025682466\n",
      "It 03250\n",
      "loss:  0.0025317317\n",
      "It 03300\n",
      "loss:  0.0024965676\n",
      "It 03350\n",
      "loss:  0.0024625324\n",
      "It 03400\n",
      "loss:  0.002429129\n",
      "It 03450\n",
      "loss:  0.002400608\n",
      "It 03500\n",
      "loss:  0.0025905415\n",
      "It 03550\n",
      "loss:  0.0023365184\n",
      "It 03600\n",
      "loss:  0.0023020362\n",
      "It 03650\n",
      "loss:  0.0022717367\n",
      "It 03700\n",
      "loss:  0.0022419505\n",
      "It 03750\n",
      "loss:  0.0022127803\n",
      "It 03800\n",
      "loss:  0.0023052292\n",
      "It 03850\n",
      "loss:  0.0021649615\n",
      "It 03900\n",
      "loss:  0.002128284\n",
      "It 03950\n",
      "loss:  0.0021010875\n",
      "It 04000\n",
      "loss:  0.002074342\n",
      "It 04050\n",
      "loss:  0.0020482622\n",
      "It 04100\n",
      "loss:  0.0033048566\n",
      "It 04150\n",
      "loss:  0.0020029536\n",
      "It 04200\n",
      "loss:  0.0019721095\n",
      "It 04250\n",
      "loss:  0.0019476197\n",
      "It 04300\n",
      "loss:  0.0019234932\n",
      "It 04350\n",
      "loss:  0.0018997189\n",
      "It 04400\n",
      "loss:  0.0023401242\n",
      "It 04450\n",
      "loss:  0.0018571266\n",
      "It 04500\n",
      "loss:  0.0018312954\n",
      "It 04550\n",
      "loss:  0.0018090823\n",
      "It 04600\n",
      "loss:  0.0017872971\n",
      "It 04650\n",
      "loss:  0.0017658062\n",
      "It 04700\n",
      "loss:  0.0017446051\n",
      "It 04750\n",
      "loss:  0.013749534\n",
      "It 04800\n",
      "loss:  0.0017061664\n",
      "It 04850\n",
      "loss:  0.0016836621\n",
      "It 04900\n",
      "loss:  0.0016636433\n",
      "It 04950\n",
      "loss:  0.0016441591\n",
      "It 05000\n",
      "loss:  0.0016249169\n",
      "It 05050\n",
      "loss:  0.0016059313\n",
      "It 05100\n",
      "loss:  0.0037623004\n",
      "It 05150\n",
      "loss:  0.0015825914\n",
      "It 05200\n",
      "loss:  0.0015512339\n",
      "It 05250\n",
      "loss:  0.0015334731\n",
      "It 05300\n",
      "loss:  0.0015159699\n",
      "It 05350\n",
      "loss:  0.001498668\n",
      "It 05400\n",
      "loss:  0.023722434\n",
      "It 05450\n",
      "loss:  0.0014936134\n",
      "It 05500\n",
      "loss:  0.0014494362\n",
      "It 05550\n",
      "loss:  0.0014326983\n",
      "It 05600\n",
      "loss:  0.0014167867\n",
      "It 05650\n",
      "loss:  0.001401042\n",
      "It 05700\n",
      "loss:  0.0013855733\n",
      "It 05750\n",
      "loss:  0.0016931951\n",
      "It 05800\n",
      "loss:  0.001364192\n",
      "It 05850\n",
      "loss:  0.0013407518\n",
      "It 05900\n",
      "loss:  0.0013261919\n",
      "It 05950\n",
      "loss:  0.0013118132\n",
      "It 06000\n",
      "loss:  0.0012976435\n",
      "It 06050\n",
      "loss:  0.0013101444\n",
      "It 06100\n",
      "loss:  0.0012715904\n",
      "It 06150\n",
      "loss:  0.0012568749\n",
      "It 06200\n",
      "loss:  0.0012435915\n",
      "It 06250\n",
      "loss:  0.0012304797\n",
      "It 06300\n",
      "loss:  0.0012174794\n",
      "It 06350\n",
      "loss:  0.0060801\n",
      "It 06400\n",
      "loss:  0.0012144699\n",
      "It 06450\n",
      "loss:  0.0011802084\n",
      "It 06500\n",
      "loss:  0.0011681189\n",
      "It 06550\n",
      "loss:  0.0011561333\n",
      "It 06600\n",
      "loss:  0.0011442384\n",
      "It 06650\n",
      "loss:  0.0029442145\n",
      "It 06700\n",
      "loss:  0.0011359982\n",
      "It 06750\n",
      "loss:  0.0011104215\n",
      "It 06800\n",
      "loss:  0.0010992505\n",
      "It 06850\n",
      "loss:  0.0010883132\n",
      "It 06900\n",
      "loss:  0.0010774424\n",
      "It 06950\n",
      "loss:  0.0018685665\n",
      "It 07000\n",
      "loss:  0.0010564777\n",
      "It 07050\n",
      "loss:  0.0010467736\n",
      "It 07100\n",
      "loss:  0.001036397\n",
      "It 07150\n",
      "loss:  0.0010264388\n",
      "It 07200\n",
      "loss:  0.001016531\n",
      "It 07250\n",
      "loss:  0.0010344542\n",
      "It 07300\n",
      "loss:  0.0013669477\n",
      "It 07350\n",
      "loss:  0.0009887594\n",
      "It 07400\n",
      "loss:  0.0009789283\n",
      "It 07450\n",
      "loss:  0.00096984487\n",
      "It 07500\n",
      "loss:  0.00096079154\n",
      "It 07550\n",
      "loss:  0.0009517889\n",
      "It 07600\n",
      "loss:  0.0009816937\n",
      "It 07650\n",
      "loss:  0.0009470339\n",
      "It 07700\n",
      "loss:  0.0009265372\n",
      "It 07750\n",
      "loss:  0.0009183395\n",
      "It 07800\n",
      "loss:  0.00091015466\n",
      "It 07850\n",
      "loss:  0.0009019861\n",
      "It 07900\n",
      "loss:  0.00089948147\n",
      "It 07950\n",
      "loss:  0.0015110011\n",
      "It 08000\n",
      "loss:  0.0008785571\n",
      "It 08050\n",
      "loss:  0.0008709994\n",
      "It 08100\n",
      "loss:  0.00086351\n",
      "It 08150\n",
      "loss:  0.00085601857\n",
      "It 08200\n",
      "loss:  0.0008485719\n",
      "It 08250\n",
      "loss:  0.0027171317\n",
      "It 08300\n",
      "loss:  0.0008401414\n",
      "It 08350\n",
      "loss:  0.00082786137\n",
      "It 08400\n",
      "loss:  0.0008211228\n",
      "It 08450\n",
      "loss:  0.0008143831\n",
      "It 08500\n",
      "loss:  0.00080763194\n",
      "It 08550\n",
      "loss:  0.0008008773\n",
      "It 08600\n",
      "loss:  0.00091398205\n",
      "It 08650\n",
      "loss:  0.000805545\n",
      "It 08700\n",
      "loss:  0.00078211556\n",
      "It 08750\n",
      "loss:  0.00077602087\n",
      "It 08800\n",
      "loss:  0.0007699095\n",
      "It 08850\n",
      "loss:  0.0007637792\n",
      "It 08900\n",
      "loss:  0.0007761564\n",
      "It 08950\n",
      "loss:  0.00075451704\n",
      "It 09000\n",
      "loss:  0.00074848067\n",
      "It 09050\n",
      "loss:  0.0007406408\n",
      "It 09100\n",
      "loss:  0.00073501683\n",
      "It 09150\n",
      "loss:  0.00072937936\n",
      "It 09200\n",
      "loss:  0.00080395007\n",
      "It 09250\n",
      "loss:  0.0010841568\n",
      "It 09300\n",
      "loss:  0.00071427063\n",
      "It 09350\n",
      "loss:  0.0007081384\n",
      "It 09400\n",
      "loss:  0.00070295046\n",
      "It 09450\n",
      "loss:  0.0006977497\n",
      "It 09500\n",
      "loss:  0.001135536\n",
      "It 09550\n",
      "loss:  0.00083931995\n",
      "It 09600\n",
      "loss:  0.0006846005\n",
      "It 09650\n",
      "loss:  0.00067821366\n",
      "It 09700\n",
      "loss:  0.00067344063\n",
      "It 09750\n",
      "loss:  0.00066864066\n",
      "It 09800\n",
      "loss:  0.00067161175\n",
      "It 09850\n",
      "loss:  0.0012561481\n",
      "It 09900\n",
      "loss:  0.0006579471\n",
      "It 09950\n",
      "loss:  0.0006507215\n",
      "It 10000\n",
      "loss:  0.0006463672\n",
      "It 10050\n",
      "loss:  0.0006419969\n",
      "It 10100\n",
      "loss:  0.00063759333\n",
      "It 10150\n",
      "loss:  0.0042816345\n",
      "It 10200\n",
      "loss:  0.00083268795\n",
      "It 10250\n",
      "loss:  0.00062550965\n",
      "It 10300\n",
      "loss:  0.0006211956\n",
      "It 10350\n",
      "loss:  0.00061717746\n",
      "It 10400\n",
      "loss:  0.0006131328\n",
      "It 10450\n",
      "loss:  0.0006091491\n",
      "It 10500\n",
      "loss:  0.0018392588\n",
      "It 10550\n",
      "loss:  0.00060269143\n",
      "It 10600\n",
      "loss:  0.00059772807\n",
      "It 10650\n",
      "loss:  0.00059396005\n",
      "It 10700\n",
      "loss:  0.0005901778\n",
      "It 10750\n",
      "loss:  0.00058686634\n",
      "It 10800\n",
      "loss:  0.00088720943\n",
      "It 10850\n",
      "loss:  0.00058442395\n",
      "It 10900\n",
      "loss:  0.00057587953\n",
      "It 10950\n",
      "loss:  0.0005723939\n",
      "It 11000\n",
      "loss:  0.00056888215\n",
      "It 11050\n",
      "loss:  0.0005653375\n",
      "It 11100\n",
      "loss:  0.0006227546\n",
      "It 11150\n",
      "loss:  0.00059633015\n",
      "It 11200\n",
      "loss:  0.0005558208\n",
      "It 11250\n",
      "loss:  0.00055231154\n",
      "It 11300\n",
      "loss:  0.0005490876\n",
      "It 11350\n",
      "loss:  0.00054582814\n",
      "It 11400\n",
      "loss:  0.0005425421\n",
      "It 11450\n",
      "loss:  0.0031278015\n",
      "It 11500\n",
      "loss:  0.0005536994\n",
      "It 11550\n",
      "loss:  0.00053335796\n",
      "It 11600\n",
      "loss:  0.0005303345\n",
      "It 11650\n",
      "loss:  0.0005272765\n",
      "It 11700\n",
      "loss:  0.0005241832\n",
      "It 11750\n",
      "loss:  0.0041655125\n",
      "It 11800\n",
      "loss:  0.0005330137\n",
      "It 11850\n",
      "loss:  0.0005161441\n",
      "It 11900\n",
      "loss:  0.0005128508\n",
      "It 11950\n",
      "loss:  0.0005100488\n",
      "It 12000\n",
      "loss:  0.00050721696\n",
      "It 12050\n",
      "loss:  0.0005043511\n",
      "It 12100\n",
      "loss:  0.0075038895\n",
      "It 12150\n",
      "loss:  0.0006283315\n",
      "It 12200\n",
      "loss:  0.0004972065\n",
      "It 12250\n",
      "loss:  0.00049358053\n",
      "It 12300\n",
      "loss:  0.0004909082\n",
      "It 12350\n",
      "loss:  0.0004882062\n",
      "It 12400\n",
      "loss:  0.0007082239\n",
      "It 12450\n",
      "loss:  0.00067868666\n",
      "It 12500\n",
      "loss:  0.0004807786\n",
      "It 12550\n",
      "loss:  0.0004780387\n",
      "It 12600\n",
      "loss:  0.00047553107\n",
      "It 12650\n",
      "loss:  0.00047298992\n",
      "It 12700\n",
      "loss:  0.00047052957\n",
      "It 12750\n",
      "loss:  0.00055730296\n",
      "It 12800\n",
      "loss:  0.00046675414\n",
      "It 12850\n",
      "loss:  0.00046336933\n",
      "It 12900\n",
      "loss:  0.0004609483\n",
      "It 12950\n",
      "loss:  0.0004585501\n",
      "It 13000\n",
      "loss:  0.00045612326\n",
      "It 13050\n",
      "loss:  0.00066124403\n",
      "It 13100\n",
      "loss:  0.0004520185\n",
      "It 13150\n",
      "loss:  0.000449391\n",
      "It 13200\n",
      "loss:  0.00044711778\n",
      "It 13250\n",
      "loss:  0.00044484835\n",
      "It 13300\n",
      "loss:  0.0004425509\n",
      "It 13350\n",
      "loss:  0.010642478\n",
      "It 13400\n",
      "loss:  0.00045487826\n",
      "It 13450\n",
      "loss:  0.00043609404\n",
      "It 13500\n",
      "loss:  0.0004339587\n",
      "It 13550\n",
      "loss:  0.00043179755\n",
      "It 13600\n",
      "loss:  0.00042961218\n",
      "It 13650\n",
      "loss:  0.007695278\n",
      "It 13700\n",
      "loss:  0.0004434356\n",
      "It 13750\n",
      "loss:  0.00042359534\n",
      "It 13800\n",
      "loss:  0.0004214534\n",
      "It 13850\n",
      "loss:  0.00041940002\n",
      "It 13900\n",
      "loss:  0.00041732303\n",
      "It 13950\n",
      "loss:  0.0033052682\n",
      "It 14000\n",
      "loss:  0.00061652355\n",
      "It 14050\n",
      "loss:  0.00041190046\n",
      "It 14100\n",
      "loss:  0.0004095829\n",
      "It 14150\n",
      "loss:  0.00040763846\n",
      "It 14200\n",
      "loss:  0.00040568088\n",
      "It 14250\n",
      "loss:  0.00040370267\n",
      "It 14300\n",
      "loss:  0.0021659515\n",
      "It 14350\n",
      "loss:  0.00041201717\n",
      "It 14400\n",
      "loss:  0.00039832504\n",
      "It 14450\n",
      "loss:  0.00039636993\n",
      "It 14500\n",
      "loss:  0.00039450623\n",
      "It 14550\n",
      "loss:  0.0003926187\n",
      "It 14600\n",
      "loss:  0.00069947273\n",
      "It 14650\n",
      "loss:  0.00043102476\n",
      "It 14700\n",
      "loss:  0.00038753386\n",
      "It 14750\n",
      "loss:  0.00038563594\n",
      "It 14800\n",
      "loss:  0.0003838706\n",
      "It 14850\n",
      "loss:  0.00038208673\n",
      "It 14900\n",
      "loss:  0.0003804012\n",
      "It 14950\n",
      "loss:  0.0011005447\n",
      "It 15000\n",
      "loss:  0.00037937178\n",
      "It 15050\n",
      "loss:  0.0003753539\n",
      "It 15100\n",
      "loss:  0.00037365637\n",
      "It 15150\n",
      "loss:  0.00037195705\n",
      "It 15200\n",
      "loss:  0.00037023757\n",
      "It 15250\n",
      "loss:  0.004233958\n",
      "It 15300\n",
      "loss:  0.00038128402\n",
      "It 15350\n",
      "loss:  0.00036558413\n",
      "It 15400\n",
      "loss:  0.00036382984\n",
      "It 15450\n",
      "loss:  0.00036219246\n",
      "It 15500\n",
      "loss:  0.0003605389\n",
      "It 15550\n",
      "loss:  0.0034954452\n",
      "It 15600\n",
      "loss:  0.0003655587\n",
      "It 15650\n",
      "loss:  0.00035605373\n",
      "It 15700\n",
      "loss:  0.00035446588\n",
      "It 15750\n",
      "loss:  0.00035290595\n",
      "It 15800\n",
      "loss:  0.00035132855\n",
      "It 15850\n",
      "loss:  0.023169948\n",
      "It 15900\n",
      "loss:  0.00035491574\n",
      "It 15950\n",
      "loss:  0.00034711367\n",
      "It 16000\n",
      "loss:  0.00034543817\n",
      "It 16050\n",
      "loss:  0.00034393425\n",
      "It 16100\n",
      "loss:  0.00034241634\n",
      "It 16150\n",
      "loss:  0.020997234\n",
      "It 16200\n",
      "loss:  0.00044152956\n",
      "It 16250\n",
      "loss:  0.0003386126\n",
      "It 16300\n",
      "loss:  0.0003368023\n",
      "It 16350\n",
      "loss:  0.00033536245\n",
      "It 16400\n",
      "loss:  0.00033390705\n",
      "It 16450\n",
      "loss:  0.00040757327\n",
      "It 16500\n",
      "loss:  0.00043390054\n",
      "It 16550\n",
      "loss:  0.00033208053\n",
      "It 16600\n",
      "loss:  0.00032849947\n",
      "It 16650\n",
      "loss:  0.00032711535\n",
      "It 16700\n",
      "loss:  0.00032572242\n",
      "It 16750\n",
      "loss:  0.00032468082\n",
      "It 16800\n",
      "loss:  0.0014339694\n",
      "It 16850\n",
      "loss:  0.0003220357\n",
      "It 16900\n",
      "loss:  0.00032048466\n",
      "It 16950\n",
      "loss:  0.00031915758\n",
      "It 17000\n",
      "loss:  0.00031781563\n",
      "It 17050\n",
      "loss:  0.00031854954\n",
      "It 17100\n",
      "loss:  0.00032583022\n",
      "It 17150\n",
      "loss:  0.00031512798\n",
      "It 17200\n",
      "loss:  0.00031275552\n",
      "It 17250\n",
      "loss:  0.00031147007\n",
      "It 17300\n",
      "loss:  0.0003101726\n",
      "It 17350\n",
      "loss:  0.0012840533\n",
      "It 17400\n",
      "loss:  0.0005166376\n",
      "It 17450\n",
      "loss:  0.00030775391\n",
      "It 17500\n",
      "loss:  0.00030533053\n",
      "It 17550\n",
      "loss:  0.00030408887\n",
      "It 17600\n",
      "loss:  0.00030282888\n",
      "It 17650\n",
      "loss:  0.00058808224\n",
      "It 17700\n",
      "loss:  0.00030285536\n",
      "It 17750\n",
      "loss:  0.00030140716\n",
      "It 17800\n",
      "loss:  0.00029824744\n",
      "It 17850\n",
      "loss:  0.0002970788\n",
      "It 17900\n",
      "loss:  0.00029589335\n",
      "It 17950\n",
      "loss:  0.0002946927\n",
      "It 18000\n",
      "loss:  0.0012743432\n",
      "It 18050\n",
      "loss:  0.0005269699\n",
      "It 18100\n",
      "loss:  0.00029261946\n",
      "It 18150\n",
      "loss:  0.00029021228\n",
      "It 18200\n",
      "loss:  0.00028906084\n",
      "It 18250\n",
      "loss:  0.0002878994\n",
      "It 18300\n",
      "loss:  0.001148652\n",
      "It 18350\n",
      "loss:  0.00036139935\n",
      "It 18400\n",
      "loss:  0.00028613812\n",
      "It 18450\n",
      "loss:  0.0002836318\n",
      "It 18500\n",
      "loss:  0.00028253612\n",
      "It 18550\n",
      "loss:  0.00028141984\n",
      "It 18600\n",
      "loss:  0.0002805589\n",
      "It 18650\n",
      "loss:  0.0011635417\n",
      "It 18700\n",
      "loss:  0.00027909008\n",
      "It 18750\n",
      "loss:  0.00027719754\n",
      "It 18800\n",
      "loss:  0.0002761333\n",
      "It 18850\n",
      "loss:  0.00027506056\n",
      "It 18900\n",
      "loss:  0.00027403617\n",
      "It 18950\n",
      "loss:  0.00032825235\n",
      "It 19000\n",
      "loss:  0.00027372374\n",
      "It 19050\n",
      "loss:  0.0002711374\n",
      "It 19100\n",
      "loss:  0.00027007447\n",
      "It 19150\n",
      "loss:  0.00026905446\n",
      "It 19200\n",
      "loss:  0.00026801063\n",
      "It 19250\n",
      "loss:  0.00028602532\n",
      "It 19300\n",
      "loss:  0.0002701486\n",
      "It 19350\n",
      "loss:  0.00026513138\n",
      "It 19400\n",
      "loss:  0.00026408906\n",
      "It 19450\n",
      "loss:  0.0002630955\n",
      "It 19500\n",
      "loss:  0.00026208383\n",
      "It 19550\n",
      "loss:  0.0004514009\n",
      "It 19600\n",
      "loss:  0.0005157108\n",
      "It 19650\n",
      "loss:  0.0002595797\n",
      "It 19700\n",
      "loss:  0.00025844164\n",
      "It 19750\n",
      "loss:  0.00025751282\n",
      "It 19800\n",
      "loss:  0.00025656723\n",
      "It 19850\n",
      "loss:  0.0002556067\n",
      "It 19900\n",
      "loss:  0.0019035311\n",
      "It 19950\n",
      "loss:  0.0004993306\n",
      "It 20000\n",
      "loss:  0.00025329797\n",
      "\n",
      "Computation time: 61.783246755599976 seconds\n"
     ]
    }
   ],
   "source": [
    "mlp = init_model(1, 2)\n",
    "optim=tf.keras.optimizers.Adam(learning_rate=0.5)\n",
    "\n",
    "train(mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3772153/380415013.py:14: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1+np.exp(-C*x+C*t+C/2))\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_21 (Dense)            (None, 2)                 6         \n",
      "                                                                 \n",
      " dense_22 (Dense)            (None, 1)                 3         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 9 (36.00 Byte)\n",
      "Trainable params: 9 (36.00 Byte)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def model_summary(mlp):\n",
    "    t0=0\n",
    "    x=np.linspace(0,3)\n",
    "    t=np.zeros((len(x), 2))\n",
    "    t[:, 1] = x\n",
    "    t[:,0] = t0    \n",
    "\n",
    "    fig, ax = plt.subplots(1, 4)\n",
    "\n",
    "    t=np.linspace(tmin, tmax, 4)\n",
    "    x = np.linspace(xmin, xmax, 40)\n",
    "\n",
    "    concat = np.zeros((len(x),2))\n",
    "    concat[:, 1] = x\n",
    "\n",
    "    def plot(i,t):\n",
    "        concat[:,0]= t\n",
    "        ax[i].plot(x, mlp(concat), color=\"red\", label=f\"Tiempo={t}\")\n",
    "        ax[i].plot(x, exact_sol(concat), color =\"green\", label=f\"Tiempo={t}\")\n",
    "        \n",
    "\n",
    "    for i in range(4):\n",
    "        plot(i, t[i])\n",
    "\n",
    "\n",
    "    plt.show()\n",
    "    mlp.summary()\n",
    "\n",
    "model_summary(mlp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
